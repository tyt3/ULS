{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to install the libraries/packages required to run the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">You only need to run the cell below the first time you use this notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the libraries and modules we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "import nltk\n",
    "from nltk import Text, word_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below, there are a few different methods to process and create concordances for documents using the NLTK and spaCy Python libraries. Using the spaCy methods below, you can save your concordances to CSV files for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning' role='alert'>\n",
    "<strong>Working with document files</strong>\n",
    "   <ul>\n",
    "       <li>By default, this notebook uses an example corpus comprising the five volumes of The Works of Edgar Allan Poe collection, downloaded from <a href='https://gutenberg.org/ebooks/search/?query=%22the+works+of+edgar+allan+poe%22&submit_search=Go%21' target='blank'>Project Gutenberg</a>. You can use your own corpus files by adding them to a new folder/directory wherever this notebook is saved, like the \"example-corpus\" directory. In that case, you should make sure to update any reference to the corpus directory and/or corpus files to reflect those that you've added. In the <a href='#Keyword-in-Context-(KWIC)-Concordance-with-NLTK'>Keyword-in-Context-(KWIC)-Concordance-with-NLTK</a> and <a href='#With-a-single-document'>Sentences Concordance with spaCy: With a single document</a> sections, you will update the <code>file</code> parameter of the <a href='https://docs.python.org/3/library/functions.html#open' target='_blank'><code>open()</code> function</a> in the first code cell. In the <a href='#With-an-entire-corpus'>Sentences Concordance with spaCy: With an entire corpus</a> section, you will need to update the value for the <code>corpus_dir</code> variable in the first code cell. There are messages in blue boxes to flag when and where to make these updates to the code.\n",
    "       </li>\n",
    "       <li>Make sure that the document you're using is a plain text file (*.txt) that is encoded in UTF-8 (an encoding system for Unicode). You can easily check the encoding of plain text files by opening them in the Notepad text editor program and looking in the bottom right corner of the window. You should see metadata for the file, including the encoding system, e.g., \"UTF-8 with BOM\".\n",
    "       </li>\n",
    "       <li>If your file is not encoded in UTF-8, you can update the <code>encoding</code> parameter of the <code>open()</code> function in the cells with code for opening documents (the first code cell of the first two sections) by replacing the text inside the quotation marks, as in <code>encoding='utf8'</code>. If you don't know what should replace <code>utf8</code>, check Python's <a href='https://docs.python.org/3/library/codecs.html#standard-encodings' target='_blank'>list of standard encodings</a>, find the correct encoding under the \"Aliases\" column, and use the corresponding \"Codec\".  \n",
    "       </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword in Context (KWIC) Concordance with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses a Python library called NLTK, which will enable us to create an interactive concordance to search for occurences of keywords in context (KWIC). \n",
    "\n",
    "**Resources**\n",
    "- [NLTK :: Sample usage for concordance](https://www.nltk.org/howto/concordance.html)\n",
    "- [NLTK :: nltk.text module :: concordance method](https://www.nltk.org/api/nltk.text.html#nltk.text.Text.concordance)\n",
    "- [Python concordance command in NLTK](https://newbedev.com/python-concordance-command-in-nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">If you'd like to work with a different document, make sure to update the file path in the cell below.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the document\n",
    "with open(r'example-corpus/The Works of Edgar Allan Poe — Volume 1 by Edgar Allan Poe.txt', encoding='utf8') as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process the document and create an NLTK Text object\n",
    "nltk_text = Text(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">To search a different keyword or keyword phrase, update the value of the <code>keyword</code> variable in the cell below, as follows:\n",
    "    <ul>\n",
    "        <li style='margin: 8px 0;'>\n",
    "            <i>Keyword</i>: <code>keyword = [\"keyword\"]</code> or <code>keyword = \"keyword\"</code>\n",
    "        </li>\n",
    "        <li style='margin: 8px 0;'>\n",
    "            <i>Keyword phrase</i>: <code>keyword = [\"keyword\", \"phrase\"]</code>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the keyword or keyword phrase to be searched\n",
    "keyword = [\"old\", \"lady\", \"was\"]\n",
    "\n",
    "# Generate the concordancer\n",
    "nltk_text.concordance(keyword, width=79, lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword in Sentences Concordance with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses a Python library called spaCy, which will allow us to iterate over the sentences in documents and create a list of them.\n",
    "\n",
    "**Resources**\n",
    "- [Doc · spaCy API Documentation · Doc.sents](https://spacy.io/api/doc#sents)\n",
    "- [spaCy - Doc.sents Property](https://www.tutorialspoint.com/spacy/spacy_doc_sents.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With a single document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">You can skip the step for opening the document in the cell below if you already did it in the NLTK section above.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the document\n",
    "with open(r\"example-corpus/The Works of Edgar Allan Poe — Volume 1 by Edgar Allan Poe.txt\", encoding='utf8') as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process the document and create a spaCy Doc object\n",
    "doc = nlp(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of parsed sentences in the document\n",
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random sentence\n",
    "sentences[randrange(len(sentences) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of sentences\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">To search a different keyword or keyword phrase, update the value of the <code>keyword</code> variable in the cell below, as follows:\n",
    "    <ul>\n",
    "        <li style='margin: 8px 0;'>\n",
    "            <i>Keyword</i>: <code>keyword = [\"keyword\"]</code> or <code>keyword = \"keyword\"</code>\n",
    "        </li>\n",
    "        <li style='margin: 8px 0;'>\n",
    "            <i>Keyword phrase</i>: <code>keyword = [\"keyword\", \"phrase\"]</code>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the keyword(s) to be searched\n",
    "keywords = [\"beautiful\", \"lady\", \"death\"]\n",
    "\n",
    "# Create a list to store keywords and the sentences in which they appear\n",
    "keyword_in_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for keyword in keywords:\n",
    "        if keyword in str(sentence):\n",
    "            keyword_in_sentences.append({'Keyword': keyword, 'Sentence': str(sentence).replace('\\n', ' ').strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of parsed sentences\n",
    "len(keyword_in_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of keywords and sentences into a DataFrame\n",
    "headers = ['Keyword', 'Sentence']\n",
    "\n",
    "keyword_in_sentences_df = pd.DataFrame(keyword_in_sentences)[headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows of the DataFrame\n",
    "keyword_in_sentences_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the document for the output file naming pattern\n",
    "document_name = file.name.split('/')[1].split('.')[0]\n",
    "\n",
    "# Set the name of the directory for the output file\n",
    "output_dir = 'keyword-data'\n",
    "\n",
    "# Create the directory for output files, if it doesn't already exist  \n",
    "try:\n",
    "    os.stat(output_dir)\n",
    "except:\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "keyword_in_sentences_df_csv = open('%s/keyword_in_sentences_%s.csv' % (output_dir, document_name), 'w', encoding='utf-8', newline='')\n",
    "keyword_in_sentences_df_csv.write(keyword_in_sentences_df.to_csv(index=False))\n",
    "keyword_in_sentences_df_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With an entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section does basically the same thing as the section above, except it processes all of the documents in a specified directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">If you've added a new directory with corpus files, make sure to update the <code>corpus_dir</code> variable appropriately.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing corpus files\n",
    "corpus_dir = 'example-corpus'\n",
    "\n",
    "# Validate directory\n",
    "if not os.path.exists(corpus_dir):\n",
    "    raise Exception (\"Directory '%s' not found!\" % (corpus_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">To search a different keyword or keyword phrase, update the value of the <code>keyword</code> variable in the cell below, as follows:\n",
    "    <ul>\n",
    "        <li style='margin: 8px 0;'>\n",
    "            <i>Keyword</i>: <code>keyword = [\"keyword\"]</code> or <code>keyword = \"keyword\"</code>\n",
    "        </li>\n",
    "        <li style='margin: 8px 0;'>\n",
    "            <i>Keyword phrase</i>: <code>keyword = [\"keyword\", \"phrase\"]</code>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the keyword(s) to be searched\n",
    "keywords = [\"beautiful\", \"lady\", \"death\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store keywords and the sentences in which they appear\n",
    "keyword_in_sentences = []\n",
    "\n",
    "# Iterate through all files in the corpus directory\n",
    "for filename in os.listdir(corpus_dir):\n",
    "    \n",
    "    # Open the document\n",
    "    if filename.endswith('.txt'):\n",
    "        document_name = os.path.splitext(filename)[0]\n",
    "        with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "\n",
    "    # Process the document and create a spaCy Doc object\n",
    "    doc = nlp(document)\n",
    "\n",
    "    # Create a list of parsed sentences in the document\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    # Add keywords and the sentences in which they appear to the list\n",
    "    for sentence in sentences:\n",
    "        for keyword in keywords:\n",
    "            if keyword in str(sentence):\n",
    "                keyword_in_sentences.append({'Document': document_name, 'Keyword': keyword, 'Sentence': str(sentence).replace('\\n', ' ')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of keywords and sentences into a DataFrame\n",
    "headers = ['Document', 'Keyword', 'Sentence']\n",
    "            \n",
    "keyword_in_sentences_df = pd.DataFrame(keyword_in_sentences)[headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows of the DataFrame\n",
    "keyword_in_sentences_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the directory for the output file\n",
    "output_dir = 'keyword-data'\n",
    " \n",
    "# Create the directory for the output file, if it doesn't already exist    \n",
    "try:\n",
    "    os.stat(output_dir)\n",
    "except:\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "keyword_in_sentences_df_csv = open('%s/keyword_in_sentences_%s.csv' % (output_dir, corpus_dir), 'w', encoding='utf-8', newline='')\n",
    "keyword_in_sentences_df_csv.write(keyword_in_sentences_df.to_csv(index=False))\n",
    "keyword_in_sentences_df_csv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
