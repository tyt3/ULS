{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to set up by importing the libraries and modules necessary to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "from nltk import Text, word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below, there are a few different methods to process and create concordances for documents using the NLTK and spaCy Python libraries. Using the spaCy methods below, you can save your concordances to CSV files for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning' role='alert'>\n",
    "<strong>Working with document files</strong>\n",
    "   <ul>\n",
    "       <li>By default, this notebook uses an example corpus comprising the five volumes of The Works of Edgar Allan Poe collection, downloaded from <a href='https://gutenberg.org/ebooks/search/?query=%22the+works+of+edgar+allan+poe%22&submit_search=Go%21' target='blank'>Project Gutenberg</a>. You can use your own corpus files by adding them to a new folder/directory wherever this notebook is saved, like the \"example-corpus\" directory. In that case, you should make sure to update the <code>file</code> parameter of the <a href='https://docs.python.org/3/library/functions.html#open' target='_blank'><code>open() function</code></a> to reflect the directory and files you've added. In the <a href='#Keyword-in-Context-(KWIC)-Concordance-with-NLTK'>Keyword-in-Context-(KWIC)-Concordance-with-NLTK</a> and <a href='#With-a-single-document'>Sentences Concordance with spaCy: With a single document</a> sections, you will update this in the first code cell. In the <a href='#With-an-entire-corpus'>Sentences Concordance with spaCy: With an entire corpus</a> section, you will need to update the value for the <code>corpus_dir</code> variable.\n",
    "       </li>\n",
    "       <li>Make sure that the document you're using is a plain text file (*.txt) that is encoded in UTF-8 (an encoding system for Unicode). You can easily check the encoding of plain text files by opening them in the Notepad text editor program and looking in the bottom right corner of the window. You should see metadata for the file, including the encoding system, e.g., \"UTF-8 with BOM\".\n",
    "       </li>\n",
    "       <li>If your file is not encoded in UTF-8, you can update the <code>encoding</code> parameter of the <code>open()</code> function in the cells with code for opening documents (the first code cell of the first two sections) by replacing the text inside the quotation marks, as in <code>encoding='utf8'</code>. If you don't know what should replace <code>utf8</code>, check Python's <a href='https://docs.python.org/3/library/codecs.html#standard-encodings' target='_blank'>list of standard encodings</a>, find the correct encoding under the \"Aliases\" column, and use the corresponding \"Codec\".  \n",
    "       </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword in Context (KWIC) Concordance with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses a Python library called NLTK, which will enable us to create an interactive concordance to search for occurences of keywords in context (KWIC). \n",
    "\n",
    "**Resources**\n",
    "- [NLTK :: Sample usage for concordance](https://www.nltk.org/howto/concordance.html)\n",
    "- [NLTK :: nltk.text module :: concordance method](https://www.nltk.org/api/nltk.text.html#nltk.text.Text.concordance)\n",
    "- [Python concordance command in NLTK](https://newbedev.com/python-concordance-command-in-nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the document\n",
    "with open(r'example-corpus\\The Works of Edgar Allan Poe — Volume 1 by Edgar Allan Poe.txt', encoding='utf8') as f:\n",
    "    document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an NLTK Text object\n",
    "nltk_text = Text(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the keyword to be searched\n",
    "keyword = [\"old\", \"lady\"]\n",
    "\n",
    "# Generate the concordancer\n",
    "nltk_text.concordance(keyword, width=79, lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword in Sentences Concordance with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses a Python library called spaCy, which will allow us to iterate over the sentences in documents and create a list of them.\n",
    "\n",
    "**Resources**\n",
    "- [Doc · spaCy API Documentation · Doc.sents](https://spacy.io/api/doc#sents)\n",
    "- [spaCy - Doc.sents Property](https://www.tutorialspoint.com/spacy/spacy_doc_sents.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With a single document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">You can skip the step for opening the document in the cell below if you already did it in the NLTK section above.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the document\n",
    "with open(r\"example-corpus\\The Works of Edgar Allan Poe — Volume 1 by Edgar Allan Poe.txt\", encoding='utf8') as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a spaCy Doc object\n",
    "doc = nlp(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of parsed sentences in the document\n",
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random sentence\n",
    "sentences[randrange(len(sentences) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of sentences\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the keyword(s) to be searched\n",
    "keywords = [\"beautiful\", \"lady\", \"death\"]\n",
    "\n",
    "# Create a list to store keywords and the sentences in which they appear\n",
    "keyword_in_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for keyword in keywords:\n",
    "        if keyword in str(sentence):\n",
    "            keyword_in_sentences.append({'Keyword': keyword, 'Sentence': str(sentence).replace('\\n', ' ')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of parsed sentences\n",
    "len(keyword_in_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of keywords and sentences into a DataFrame\n",
    "headers = ['Keyword', 'Sentence']\n",
    "\n",
    "keyword_in_sentences_df = pd.DataFrame(keyword_in_sentences)[headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows of the DataFrame\n",
    "keyword_in_sentences_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "document_name = file.name.split('\\\\')[1].split('.')[0]\n",
    "\n",
    "keyword_in_sentences_df_csv = open('keyword_in_sentences_%s.csv' % (document_name), 'w', encoding='utf-8', newline='')\n",
    "keyword_in_sentences_df_csv.write(keyword_in_sentences_df.to_csv(index=False))\n",
    "keyword_in_sentences_df_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With an entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing corpus files\n",
    "corpus_dir = 'example-corpus'\n",
    "\n",
    "# Set the keyword(s) to be searched\n",
    "keywords = [\"beautiful\", \"lady\", \"death\"]\n",
    "\n",
    "# Create a list to store keywords and the sentences in which they appear\n",
    "keyword_in_sentences = []\n",
    "\n",
    "# Iterate through all files in the corpus directory\n",
    "for filename in os.listdir(corpus_dir):\n",
    "    \n",
    "    # Open the document\n",
    "    if filename.endswith('.txt'):\n",
    "        document_name = os.path.splitext(filename)[0]\n",
    "        with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "\n",
    "    # Create a processed Doc object\n",
    "    doc = nlp(document)\n",
    "\n",
    "    # Create a list of parsed sentences in the document\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    # Add keywords and the sentences in which they appear to the list\n",
    "    for sentence in sentences:\n",
    "        for keyword in keywords:\n",
    "            if keyword in str(sentence):\n",
    "                keyword_in_sentences.append({'Document': document_name, 'Keyword': keyword, 'Sentence': str(sentence).replace('\\n', ' ')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of keywords and sentences into a DataFrame\n",
    "headers = ['Document', 'Keyword', 'Sentence']\n",
    "            \n",
    "keyword_in_sentences_df = pd.DataFrame(keyword_in_sentences)[headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows of the DataFrame\n",
    "keyword_in_sentences_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "keyword_in_sentences_df_csv = open('keyword_in_sentences_example-corpus.csv', 'w', encoding='utf-8', newline='')\n",
    "keyword_in_sentences_df_csv.write(keyword_in_sentences_df.to_csv(index=False))\n",
    "keyword_in_sentences_df_csv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
